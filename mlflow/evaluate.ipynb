{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d33f82-bf67-412f-8813-e77d65491679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"BASE_MODEL_NAME\", \"\", \"Base model\")\n",
    "dbutils.widgets.text(\"NEW_MODEL_NAME\", \"\", \"Train name\")\n",
    "dbutils.widgets.text(\"CHALLENGER_VERSION\", \"\", \"Challenger version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d4aa42e-6c48-4fe6-96e3-bac229b0d7eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers tf-keras torch evaluate pylint radon pyyaml bitsandbytes accelerate git+https://github.com/google-research/bleurt.git\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27064b79-0631-41e4-889a-a8a330f84733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Get the number of available CPU threads\n",
    "num_threads = os.cpu_count() or multiprocessing.cpu_count()\n",
    "print(f\"Number of available threads: {num_threads}\")\n",
    "\n",
    "# Set the number of threads for OpenMP and MKL\n",
    "os.environ[\"OMP_NUM_THREADS\"] = f\"{num_threads}\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = f\"{num_threads}\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = f\"{num_threads}\"\n",
    "os.environ[\"BLAS_NUM_THREADS\"] = f\"{num_threads}\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "BASE_MODEL_NAME = dbutils.widgets.get(\"BASE_MODEL_NAME\")\n",
    "NEW_MODEL_NAME = dbutils.widgets.get(\"NEW_MODEL_NAME\")\n",
    "CHALLENGER_VERSION = dbutils.widgets.get(\"CHALLENGER_VERSION\")\n",
    "\n",
    "print(f\"Starting evaluation for model: {NEW_MODEL_NAME}\")\n",
    "print(f\"Challenger version: {CHALLENGER_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41dfdd8e-6502-4d5e-9511-40ffc19cecce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import ast\n",
    "import re\n",
    "\n",
    "\n",
    "def generate_from_original(model, tokenizer, original_code, device=\"cpu\"):\n",
    "    prompt = f\"\"\"\n",
    "You are a coding assistant. Your task is to refactore a provided Python code.\n",
    "Respond ONLY with a clean and efficient Python code and nothing else.\n",
    "No comments or other text. No explanations. No markdown.<|EOT|>\n",
    "\n",
    "User:\n",
    "{original_code}\n",
    "<|EOT|>\n",
    "\n",
    "Assistant:\n",
    "```python\n",
    "\"\"\"\n",
    "    print(\"Tokenizing\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    print(\"Generating\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=32021\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        output_ids[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Extract code block from response\n",
    "    code_match = re.search(r\"(.*?)```\", response, re.DOTALL)\n",
    "    code = code_match.group(1).strip() if code_match else response\n",
    "\n",
    "    # print(f\"Generated code:\\n{response}\")\n",
    "    print(f\"Trimmed code:\\n{code}\")\n",
    "\n",
    "    return code.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872838e0-b73c-47d6-bf06-178d6a98d9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "py_tests = Path(\"./tests.yaml\").read_text()\n",
    "py_tests = yaml.safe_load(py_tests)[\"tests\"]\n",
    "print(f\"Successfully loaded {len(py_tests)} tests\")\n",
    "\n",
    "\n",
    "def run_tests(model_uri):\n",
    "    print(f\"Running genration for model at: {model_uri}\")\n",
    "\n",
    "    loaded_model_pipeline = mlflow.transformers.load_model(\n",
    "        model_uri,\n",
    "        # torch_dtype=torch.float16,\n",
    "        device_map=\"cpu\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    model = loaded_model_pipeline.model\n",
    "    tokenizer = loaded_model_pipeline.tokenizer\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    results = []\n",
    "    for index, test in enumerate(py_tests):\n",
    "        refactored_code = test[\"refactored\"]\n",
    "        original_code = test[\"original\"]\n",
    "\n",
    "        print(f\"Generating code for test {index + 1}...\")\n",
    "        generated_code = generate_from_original(model, tokenizer, original_code)\n",
    "\n",
    "        results.append({\n",
    "            \"original\": original_code,\n",
    "            \"generated\": generated_code,\n",
    "        })\n",
    "    \n",
    "    del model\n",
    "    del tokenizer\n",
    "    del loaded_model_pipeline\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72db005-1a2e-48f3-9cad-d10857cb094b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_benchmark(tests, metric, model_uri):\n",
    "    print(f\"Running banchmark for model at: {model_uri}\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for index, test in enumerate(tests):\n",
    "        original_code = test[\"original\"]\n",
    "        generated_code = test[\"generated\"]\n",
    "\n",
    "        print(f\"Computing benchmark score for test {index + 1}...\")\n",
    "        benchmark_score = metric.compute(\n",
    "            predictions=[generated_code],\n",
    "            references=[original_code]\n",
    "        )['scores'][0]\n",
    "        \n",
    "        scores.append(benchmark_score)\n",
    "    \n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average benchmark score: {average_score}\")\n",
    "\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352bcd9b-ab55-41bf-b3ba-8c1fe24ada88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "challenger_uri = f\"models:/{NEW_MODEL_NAME}/{CHALLENGER_VERSION}\"\n",
    "challenger_tests = run_tests(challenger_uri)\n",
    "\n",
    "latest_prod = client.get_latest_versions(NEW_MODEL_NAME, stages=[\"Production\"])\n",
    "\n",
    "if latest_prod:\n",
    "    champion = latest_prod[0]\n",
    "    champion_version = champion.version\n",
    "    print(f\"Found Champion: version {champion_version}\")\n",
    "    champion_uri = f\"models:/{NEW_MODEL_NAME}/{champion_version}\"\n",
    "    champion_tests = run_tests(champion_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36052a44-cbac-4620-b71a-efaf51d0c998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "bleurt = evaluate.load(\"bleurt\", module_type=\"metric\", config_name=\"BLEURT-20\")\n",
    "challenger_score = run_benchmark(challenger_tests, bleurt, challenger_uri)\n",
    "champion_score = run_benchmark(champion_tests, bleurt, champion_uri) if latest_prod else None\n",
    "del bleurt\n",
    "\n",
    "\n",
    "# Return scores\n",
    "dbutils.notebook.exit({\n",
    "    \"champion_score\": champion_score,\n",
    "    \"challenger_score\": challenger_score\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "evaluate",
   "widgets": {
    "BASE_MODEL_NAME": {
     "currentValue": "deepseek-ai/deepseek-coder-1.3b-instruct",
     "nuid": "bde21e7f-6fb8-45ee-af8e-5b244ac5fb0c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Base model",
      "name": "BASE_MODEL_NAME",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "Base model",
      "name": "BASE_MODEL_NAME",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "CHALLENGER_VERSION": {
     "currentValue": "8",
     "nuid": "5c5ae4a6-6f79-4cf6-9b4d-1d9c52cb6db8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Challenger version",
      "name": "CHALLENGER_VERSION",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "Challenger version",
      "name": "CHALLENGER_VERSION",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "NEW_MODEL_NAME": {
     "currentValue": "mlops_coder",
     "nuid": "7081417c-dadb-49f1-8bb3-1bd1e1f0d295",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Train name",
      "name": "NEW_MODEL_NAME",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "Train name",
      "name": "NEW_MODEL_NAME",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
